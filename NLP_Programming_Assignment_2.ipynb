{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Start timing the whole notebook\n",
    "notebook_start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully\n",
      "Datasets merged successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd \n",
    "\n",
    "# Uploading the training dataset\n",
    "df_train = pd.read_csv('trainingandtestdata/train.csv', encoding='latin-1', header=None)\n",
    "df_train.columns = ['Label', 'Text1', 'TimeStamp', 'Meta1', 'Meta2', 'MainText']\n",
    "\n",
    "# Uploading the testing dataset\n",
    "df_test = pd.read_csv('trainingandtestdata/test.csv', encoding='latin-1', header=None)\n",
    "df_test.columns = ['Label', 'Text1', 'TimeStamp', 'Meta1', 'Meta2', 'MainText']\n",
    "\n",
    "print(\"Datasets loaded successfully\")\n",
    "\n",
    "#Merging the two datasets\n",
    "\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "print(\"Datasets merged successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Documents for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the dataset:  1600498\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text1</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Meta1</th>\n",
       "      <th>Meta2</th>\n",
       "      <th>MainText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label       Text1                     TimeStamp     Meta1            Meta2  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                            MainText  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output the merged dataset\n",
    "number_of_documents= len(df)\n",
    "print(\"Number of documents in the dataset: \", number_of_documents)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set a specific path for nltk data\n",
    "nltk.data.path = ['/usr/local/share/nltk_data']\n",
    "\n",
    "# Re-download the 'punkt' resource\n",
    "nltk.download('punkt_tab', download_dir='/usr/local/share/nltk_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before clean:\n",
      "Number of sentences in the dataset: 2748402\n",
      "Number of words in the dataset: 26261279\n",
      "Average number of sentences per document: 1.7172167662814948\n",
      "Word count (total): 26261279\n",
      "Unique words in the dataset: 874726\n",
      "Max word length: 204\n",
      "Min sentence length: 1\n",
      "Max sentence length: 229\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(\"Before clean:\")\n",
    "# Tokenize the text into sentences\n",
    "df['before_sent_tokenized_text'] = df['MainText'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "# Tokenize the text into words\n",
    "df['before_word_tokenized_text'] = df['before_sent_tokenized_text'].apply(\n",
    "    lambda sentences: [word_tokenize(sentence) for sentence in sentences]\n",
    ")\n",
    "\n",
    "# Calculate the number of sentences\n",
    "before_number_of_sentences = df['before_sent_tokenized_text'].apply(len).sum()\n",
    "print(\"Number of sentences in the dataset:\", before_number_of_sentences)\n",
    "\n",
    "# Calculate the total word count\n",
    "before_number_of_words = df['before_word_tokenized_text'].apply(\n",
    "    lambda sentences: sum(len(sentence) for sentence in sentences)\n",
    ").sum()\n",
    "print(\"Number of words in the dataset:\", before_number_of_words)\n",
    "\n",
    "# Calculate the average number of sentences per document\n",
    "\n",
    "before_avg_sentences_per_doc = before_number_of_sentences / number_of_documents\n",
    "print(\"Average number of sentences per document:\", before_avg_sentences_per_doc)\n",
    "\n",
    "# Word count for the entire dataset\n",
    "print(\"Word count (total):\", before_number_of_words)\n",
    "\n",
    "# Unique words for the entire dataset\n",
    "before_unique_words = len(set(\n",
    "    word for word_list in df['before_word_tokenized_text'].explode() for word in word_list\n",
    "))\n",
    "print(\"Unique words in the dataset:\", before_unique_words)\n",
    "\n",
    "# Max word length\n",
    "before_max_word_length = max(\n",
    "    len(word) for word_list in df['before_word_tokenized_text'].explode() for word in word_list\n",
    ")\n",
    "print(\"Max word length:\", before_max_word_length)\n",
    "\n",
    "# Min sentence length (in words)\n",
    "before_min_sentence_length = min(\n",
    "    len(sentence) for sentence_list in df['before_word_tokenized_text'] for sentence in sentence_list\n",
    ")\n",
    "print(\"Min sentence length:\", before_min_sentence_length)\n",
    "\n",
    "# Max sentence length (in words)\n",
    "before_max_sentence_length = max(\n",
    "    len(sentence) for sentence_list in df['before_word_tokenized_text'] for sentence in sentence_list\n",
    ")\n",
    "print(\"Max sentence length:\", before_max_sentence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label                         0\n",
       "Text1                         0\n",
       "TimeStamp                     0\n",
       "Meta1                         0\n",
       "Meta2                         0\n",
       "MainText                      0\n",
       "before_sent_tokenized_text    0\n",
       "before_word_tokenized_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking df for missing values\n",
    "df.isnull().sum()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @switchfoot http://twitpic.com/2y1zl - awww, t...\n",
       "1          is upset that he can't update his facebook by ...\n",
       "2          @kenichan i dived many times for the ball. man...\n",
       "3            my whole body feels itchy and like its on fire \n",
       "4          @nationwideclass no, it's not behaving at all....\n",
       "                                 ...                        \n",
       "1600493    ask programming: latex or indesign?: submitted...\n",
       "1600494    on that note, i hate word. i hate pages. i hat...\n",
       "1600495    ahhh... back in a *real* text editing environm...\n",
       "1600496    trouble in iran, i see. hmm. iran. iran so far...\n",
       "1600497    reading the tweets coming out of iran... the w...\n",
       "Name: cleaned_text, Length: 1600498, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text preprocessing \n",
    "\n",
    "#Transforming all the text to lowercase\n",
    "import re \n",
    "df['cleaned_text'] = df['MainText'].apply(lambda x: x.lower())\n",
    "df['cleaned_text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the web address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs in the dataset: 71722\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>URL_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "      <td>[http://twitpic.com/2y1zl]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600493</th>\n",
       "      <td>ask programming: latex or indesign?: submitted...</td>\n",
       "      <td>[http://tinyurl.com/myfmf7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600494</th>\n",
       "      <td>on that note, i hate word. i hate pages. i hat...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600495</th>\n",
       "      <td>ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600496</th>\n",
       "      <td>trouble in iran, i see. hmm. iran. iran so far...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600497</th>\n",
       "      <td>reading the tweets coming out of iran... the w...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              cleaned_text  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - awww, t...   \n",
       "1        is upset that he can't update his facebook by ...   \n",
       "2        @kenichan i dived many times for the ball. man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1600493  ask programming: latex or indesign?: submitted...   \n",
       "1600494  on that note, i hate word. i hate pages. i hat...   \n",
       "1600495  ahhh... back in a *real* text editing environm...   \n",
       "1600496  trouble in iran, i see. hmm. iran. iran so far...   \n",
       "1600497  reading the tweets coming out of iran... the w...   \n",
       "\n",
       "                         URL_address  \n",
       "0         [http://twitpic.com/2y1zl]  \n",
       "1                                 []  \n",
       "2                                 []  \n",
       "3                                 []  \n",
       "4                                 []  \n",
       "...                              ...  \n",
       "1600493  [http://tinyurl.com/myfmf7]  \n",
       "1600494                           []  \n",
       "1600495                           []  \n",
       "1600496                           []  \n",
       "1600497                           []  \n",
       "\n",
       "[1600498 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract URLs from the text\n",
    "df['URL_address'] = df['cleaned_text'].apply(lambda x: re.findall(r'https?://\\S+', x))\n",
    "\n",
    "# Count the total number of URLs in the dataset\n",
    "total_urls = df['URL_address'].apply(len).sum()\n",
    "\n",
    "print(\"Total number of URLs in the dataset:\", total_urls)\n",
    "# Display the first few rows of extracted URLs for verification\n",
    "df[['cleaned_text', 'URL_address']]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URL web address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs removed successfully\n"
     ]
    }
   ],
   "source": [
    "#Removing URLs from the text\n",
    "df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub(r'https?://\\S+', '', x))\n",
    "print (\"URLs removed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of accounts detected in the dataset: 786705\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>detected_account</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@switchfoot  - awww, that's a bummer.  you sho...</td>\n",
       "      <td>[switchfoot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>[kenichan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[nationwideclass]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600493</th>\n",
       "      <td>ask programming: latex or indesign?: submitted...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600494</th>\n",
       "      <td>on that note, i hate word. i hate pages. i hat...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600495</th>\n",
       "      <td>ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600496</th>\n",
       "      <td>trouble in iran, i see. hmm. iran. iran so far...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600497</th>\n",
       "      <td>reading the tweets coming out of iran... the w...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600498 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              cleaned_text   detected_account\n",
       "0        @switchfoot  - awww, that's a bummer.  you sho...       [switchfoot]\n",
       "1        is upset that he can't update his facebook by ...                 []\n",
       "2        @kenichan i dived many times for the ball. man...         [kenichan]\n",
       "3          my whole body feels itchy and like its on fire                  []\n",
       "4        @nationwideclass no, it's not behaving at all....  [nationwideclass]\n",
       "...                                                    ...                ...\n",
       "1600493  ask programming: latex or indesign?: submitted...                 []\n",
       "1600494  on that note, i hate word. i hate pages. i hat...                 []\n",
       "1600495  ahhh... back in a *real* text editing environm...                 []\n",
       "1600496  trouble in iran, i see. hmm. iran. iran so far...                 []\n",
       "1600497  reading the tweets coming out of iran... the w...                 []\n",
       "\n",
       "[1600498 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['detected_account']= df['cleaned_text'].apply(lambda x: re.findall(r'@(\\w+)', x))\n",
    "total_accounts = df['detected_account'].apply(len).sum()\n",
    "print(\"Total number of accounts detected in the dataset:\", total_accounts)\n",
    "\n",
    "# Display the first few rows of detected accounts for verification\n",
    "df[['cleaned_text', 'detected_account']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting PhoneNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of phone numbers detected in the dataset: 1215\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>detected_phone_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1600493</th>\n",
       "      <td>ask programming: latex or indesign?: submitted...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600494</th>\n",
       "      <td>on that note, i hate word. i hate pages. i hat...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600495</th>\n",
       "      <td>ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600496</th>\n",
       "      <td>trouble in iran, i see. hmm. iran. iran so far...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600497</th>\n",
       "      <td>reading the tweets coming out of iran... the w...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              cleaned_text  \\\n",
       "1600493  ask programming: latex or indesign?: submitted...   \n",
       "1600494  on that note, i hate word. i hate pages. i hat...   \n",
       "1600495  ahhh... back in a *real* text editing environm...   \n",
       "1600496  trouble in iran, i see. hmm. iran. iran so far...   \n",
       "1600497  reading the tweets coming out of iran... the w...   \n",
       "\n",
       "        detected_phone_number  \n",
       "1600493                    []  \n",
       "1600494                    []  \n",
       "1600495                    []  \n",
       "1600496                    []  \n",
       "1600497                    []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#phone number detected \n",
    "df['detected_phone_number'] = df['cleaned_text'].apply(lambda x: re.findall(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})', x))\n",
    "total_phone_numbers = df['detected_phone_number'].apply(len).sum()\n",
    "print(\"Total number of phone numbers detected in the dataset:\", total_phone_numbers)\n",
    "\n",
    "# Display the first few rows of detected phone numbers for verification\n",
    "df[['cleaned_text', 'detected_phone_number']].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters ,numbers ,spaces, emoticons and hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total special characters detected: 6876608\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a new column with special characters removed\n",
    "df['special_character_removed'] = df['cleaned_text'].apply(lambda x: re.sub(r'[^a-z\\s]', '', x))\n",
    "\n",
    "# Calculate the number of special characters detected (original length - cleaned length)\n",
    "df['special_characters_detected'] = df['cleaned_text'].apply(len) - df['special_character_removed'].apply(len)\n",
    "\n",
    "# Get the total number of special characters detected\n",
    "total_special_characters_detected = df['special_characters_detected'].sum()\n",
    "\n",
    "print(\"Total special characters detected:\", total_special_characters_detected)\n",
    "\n",
    "\n",
    "\n",
    "#removing space \n",
    "df['special_character_removed'] = df['special_character_removed'].apply(lambda x: re.sub(r'[\\s+]', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text1</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Meta1</th>\n",
       "      <th>Meta2</th>\n",
       "      <th>MainText</th>\n",
       "      <th>before_sent_tokenized_text</th>\n",
       "      <th>before_word_tokenized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>URL_address</th>\n",
       "      <th>detected_account</th>\n",
       "      <th>detected_phone_number</th>\n",
       "      <th>special_character_removed</th>\n",
       "      <th>special_characters_detected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[@switchfoot http://twitpic.com/2y1zl - Awww, ...</td>\n",
       "      <td>[[@, switchfoot, http, :, //twitpic.com/2y1zl,...</td>\n",
       "      <td>@switchfoot  - awww, that's a bummer.  you sho...</td>\n",
       "      <td>[http://twitpic.com/2y1zl]</td>\n",
       "      <td>[switchfoot]</td>\n",
       "      <td>[]</td>\n",
       "      <td>switchfoot   awww thats a bummer  you shoulda ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is upset that he can't update his Facebook by...</td>\n",
       "      <td>[[is, upset, that, he, ca, n't, update, his, F...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[@Kenichan I dived many times for the ball., M...</td>\n",
       "      <td>[[@, Kenichan, I, dived, many, times, for, the...</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[kenichan]</td>\n",
       "      <td>[]</td>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my whole body feels itchy and like its on fire]</td>\n",
       "      <td>[[my, whole, body, feels, itchy, and, like, it...</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[@nationwideclass no, it's not behaving at all...</td>\n",
       "      <td>[[@, nationwideclass, no, ,, it, 's, not, beha...</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nationwideclass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "      <td>[Ask Programming: LaTeX or InDesign?, : submit...</td>\n",
       "      <td>[[Ask, Programming, :, LaTeX, or, InDesign, ?]...</td>\n",
       "      <td>ask programming: latex or indesign?: submitted...</td>\n",
       "      <td>[http://tinyurl.com/myfmf7]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ask programming latex or indesign submitted by...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "      <td>[On that note, I hate Word., I hate Pages., I ...</td>\n",
       "      <td>[[On, that, note, ,, I, hate, Word, .], [I, ha...</td>\n",
       "      <td>on that note, i hate word. i hate pages. i hat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>on that note i hate word i hate pages i hate l...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[Ahhh... back in a *real* text editing environ...</td>\n",
       "      <td>[[Ahhh, ..., back, in, a, *, real, *, text, ed...</td>\n",
       "      <td>ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ahhh back in a real text editing environment i...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "      <td>[Trouble in Iran, I see., Hmm., Iran., Iran so...</td>\n",
       "      <td>[[Trouble, in, Iran, ,, I, see, .], [Hmm, .], ...</td>\n",
       "      <td>trouble in iran, i see. hmm. iran. iran so far...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>trouble in iran i see hmm iran iran so far awa...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "      <td>[Reading the tweets coming out of Iran..., The...</td>\n",
       "      <td>[[Reading, the, tweets, coming, out, of, Iran,...</td>\n",
       "      <td>reading the tweets coming out of iran... the w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>reading the tweets coming out of iran the whol...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600498 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label       Text1                     TimeStamp     Meta1  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1600493      2       14072  Sun Jun 14 04:31:43 UTC 2009     latex   \n",
       "1600494      0       14073  Sun Jun 14 04:32:17 UTC 2009     latex   \n",
       "1600495      4       14074  Sun Jun 14 04:36:34 UTC 2009     latex   \n",
       "1600496      0       14075  Sun Jun 14 21:36:07 UTC 2009      iran   \n",
       "1600497      0       14076  Sun Jun 14 21:36:17 UTC 2009      iran   \n",
       "\n",
       "                   Meta2                                           MainText  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1600493          proggit  Ask Programming: LaTeX or InDesign?: submitted...   \n",
       "1600494           sam33r  On that note, I hate Word. I hate Pages. I hat...   \n",
       "1600495  iamtheonlyjosie  Ahhh... back in a *real* text editing environm...   \n",
       "1600496        plutopup7  Trouble in Iran, I see. Hmm. Iran. Iran so far...   \n",
       "1600497     captain_pete  Reading the tweets coming out of Iran... The w...   \n",
       "\n",
       "                                before_sent_tokenized_text  \\\n",
       "0        [@switchfoot http://twitpic.com/2y1zl - Awww, ...   \n",
       "1        [is upset that he can't update his Facebook by...   \n",
       "2        [@Kenichan I dived many times for the ball., M...   \n",
       "3         [my whole body feels itchy and like its on fire]   \n",
       "4        [@nationwideclass no, it's not behaving at all...   \n",
       "...                                                    ...   \n",
       "1600493  [Ask Programming: LaTeX or InDesign?, : submit...   \n",
       "1600494  [On that note, I hate Word., I hate Pages., I ...   \n",
       "1600495  [Ahhh... back in a *real* text editing environ...   \n",
       "1600496  [Trouble in Iran, I see., Hmm., Iran., Iran so...   \n",
       "1600497  [Reading the tweets coming out of Iran..., The...   \n",
       "\n",
       "                                before_word_tokenized_text  \\\n",
       "0        [[@, switchfoot, http, :, //twitpic.com/2y1zl,...   \n",
       "1        [[is, upset, that, he, ca, n't, update, his, F...   \n",
       "2        [[@, Kenichan, I, dived, many, times, for, the...   \n",
       "3        [[my, whole, body, feels, itchy, and, like, it...   \n",
       "4        [[@, nationwideclass, no, ,, it, 's, not, beha...   \n",
       "...                                                    ...   \n",
       "1600493  [[Ask, Programming, :, LaTeX, or, InDesign, ?]...   \n",
       "1600494  [[On, that, note, ,, I, hate, Word, .], [I, ha...   \n",
       "1600495  [[Ahhh, ..., back, in, a, *, real, *, text, ed...   \n",
       "1600496  [[Trouble, in, Iran, ,, I, see, .], [Hmm, .], ...   \n",
       "1600497  [[Reading, the, tweets, coming, out, of, Iran,...   \n",
       "\n",
       "                                              cleaned_text  \\\n",
       "0        @switchfoot  - awww, that's a bummer.  you sho...   \n",
       "1        is upset that he can't update his facebook by ...   \n",
       "2        @kenichan i dived many times for the ball. man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1600493  ask programming: latex or indesign?: submitted...   \n",
       "1600494  on that note, i hate word. i hate pages. i hat...   \n",
       "1600495  ahhh... back in a *real* text editing environm...   \n",
       "1600496  trouble in iran, i see. hmm. iran. iran so far...   \n",
       "1600497  reading the tweets coming out of iran... the w...   \n",
       "\n",
       "                         URL_address   detected_account detected_phone_number  \\\n",
       "0         [http://twitpic.com/2y1zl]       [switchfoot]                    []   \n",
       "1                                 []                 []                    []   \n",
       "2                                 []         [kenichan]                    []   \n",
       "3                                 []                 []                    []   \n",
       "4                                 []  [nationwideclass]                    []   \n",
       "...                              ...                ...                   ...   \n",
       "1600493  [http://tinyurl.com/myfmf7]                 []                    []   \n",
       "1600494                           []                 []                    []   \n",
       "1600495                           []                 []                    []   \n",
       "1600496                           []                 []                    []   \n",
       "1600497                           []                 []                    []   \n",
       "\n",
       "                                 special_character_removed  \\\n",
       "0        switchfoot   awww thats a bummer  you shoulda ...   \n",
       "1        is upset that he cant update his facebook by t...   \n",
       "2        kenichan i dived many times for the ball manag...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        nationwideclass no its not behaving at all im ...   \n",
       "...                                                    ...   \n",
       "1600493  ask programming latex or indesign submitted by...   \n",
       "1600494  on that note i hate word i hate pages i hate l...   \n",
       "1600495  ahhh back in a real text editing environment i...   \n",
       "1600496  trouble in iran i see hmm iran iran so far awa...   \n",
       "1600497  reading the tweets coming out of iran the whol...   \n",
       "\n",
       "         special_characters_detected  \n",
       "0                                  7  \n",
       "1                                  6  \n",
       "2                                  5  \n",
       "3                                  0  \n",
       "4                                  9  \n",
       "...                              ...  \n",
       "1600493                            9  \n",
       "1600494                            9  \n",
       "1600495                           10  \n",
       "1600496                            6  \n",
       "1600497                            6  \n",
       "\n",
       "[1600498 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Text Statstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count: 1600498\n",
      "Word count: 88585053\n",
      "Average sentence length: 55.34843092587432\n",
      "Vocabulary size: 26\n",
      "Minimum word length: 1\n",
      "Maximum word length: 125\n",
      "Minimum sentence length: 1\n",
      "Maximum sentence length: 41\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Tokenize into sentences\n",
    "df['sent_tokenized_text'] = df['special_character_removed'].apply(lambda x: sent_tokenize(x))\n",
    "\n",
    "\n",
    "\n",
    "#Tkenize into words from sentences\n",
    "df['word_tokenized_text'] = df['sent_tokenized_text'].apply(\n",
    "    lambda sentences: [word for sentence in sentences for word in word_tokenize(sentence)]\n",
    ")\n",
    "\n",
    "#Sentence count \n",
    "sentence_count = df['sent_tokenized_text'].apply(len).sum()\n",
    "print(\"Sentence count:\", sentence_count)\n",
    "\n",
    "#word count\n",
    "word_count = df['word_tokenized_text'].apply(lambda x: sum(len(sentence) for sentence in x)).sum()\n",
    "print(\"Word count:\", word_count)\n",
    "\n",
    "#Average sentence length\n",
    "total_sentences = df['sent_tokenized_text'].apply(len).sum()\n",
    "total_words = df['word_tokenized_text'].apply(lambda x: sum(len(sentence) for sentence in x)).sum()\n",
    "avg_sentence_length = total_words / total_sentences if total_sentences > 0 else 0\n",
    "print(\"Average sentence length:\", avg_sentence_length)\n",
    "\n",
    "\n",
    "#Unique words\n",
    "\n",
    "vocabulary = set(word for words in df['word_tokenized_text'].explode() for word in words)\n",
    "vocabulary_size_entire_dataset = len(vocabulary)\n",
    "print(\"Vocabulary size:\", vocabulary_size_entire_dataset)\n",
    "\n",
    "\n",
    "# Minimum and maximum word length\n",
    "min_word_length = df['word_tokenized_text'].apply(lambda x: min(len(word) for word in x)).min()\n",
    "max_word_length = df['word_tokenized_text'].apply(lambda x: max(len(word) for word in x)).max()\n",
    "\n",
    "print(f\"Minimum word length: {min_word_length}\")\n",
    "print(f\"Maximum word length: {max_word_length}\")\n",
    "\n",
    "# Minimum and maximum sentence length (in terms of word count)\n",
    "min_sentence_length = df['word_tokenized_text'].apply(len).min()\n",
    "max_sentence_length = df['word_tokenized_text'].apply(len).max()\n",
    "\n",
    "print(f\"Minimum sentence length: {min_sentence_length}\")\n",
    "print(f\"Maximum sentence length: {max_sentence_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords Removing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopWord removal\n",
    "nltk.download('stopwords', download_dir='/usr/local/share/nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before removing stopwords: 20648678\n",
      "Stopwords removed: 8371773\n",
      "Total words after removing stopwords: 12276905\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text1</th>\n",
       "      <th>TimeStamp</th>\n",
       "      <th>Meta1</th>\n",
       "      <th>Meta2</th>\n",
       "      <th>MainText</th>\n",
       "      <th>before_sent_tokenized_text</th>\n",
       "      <th>before_word_tokenized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>URL_address</th>\n",
       "      <th>detected_account</th>\n",
       "      <th>detected_phone_number</th>\n",
       "      <th>special_character_removed</th>\n",
       "      <th>special_characters_detected</th>\n",
       "      <th>sent_tokenized_text</th>\n",
       "      <th>word_tokenized_text</th>\n",
       "      <th>stopwords_removed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>[@switchfoot http://twitpic.com/2y1zl - Awww, ...</td>\n",
       "      <td>[[@, switchfoot, http, :, //twitpic.com/2y1zl,...</td>\n",
       "      <td>@switchfoot  - awww, that's a bummer.  you sho...</td>\n",
       "      <td>[http://twitpic.com/2y1zl]</td>\n",
       "      <td>[switchfoot]</td>\n",
       "      <td>[]</td>\n",
       "      <td>switchfoot   awww thats a bummer  you shoulda ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[switchfoot   awww thats a bummer  you shoulda...</td>\n",
       "      <td>[switchfoot, awww, thats, a, bummer, you, shou...</td>\n",
       "      <td>[switchfoot, awww, thats, bummer, shoulda, got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>[is upset that he can't update his Facebook by...</td>\n",
       "      <td>[[is, upset, that, he, ca, n't, update, his, F...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>6</td>\n",
       "      <td>[is upset that he cant update his facebook by ...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>[@Kenichan I dived many times for the ball., M...</td>\n",
       "      <td>[[@, Kenichan, I, dived, many, times, for, the...</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[kenichan]</td>\n",
       "      <td>[]</td>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "      <td>5</td>\n",
       "      <td>[kenichan i dived many times for the ball mana...</td>\n",
       "      <td>[kenichan, i, dived, many, times, for, the, ba...</td>\n",
       "      <td>[kenichan, dived, many, times, ball, managed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my whole body feels itchy and like its on fire]</td>\n",
       "      <td>[[my, whole, body, feels, itchy, and, like, it...</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "      <td>[my whole body feels itchy and like its on fire]</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[@nationwideclass no, it's not behaving at all...</td>\n",
       "      <td>[[@, nationwideclass, no, ,, it, 's, not, beha...</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nationwideclass]</td>\n",
       "      <td>[]</td>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "      <td>9</td>\n",
       "      <td>[nationwideclass no its not behaving at all im...</td>\n",
       "      <td>[nationwideclass, no, its, not, behaving, at, ...</td>\n",
       "      <td>[nationwideclass, behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "      <td>[Ask Programming: LaTeX or InDesign?, : submit...</td>\n",
       "      <td>[[Ask, Programming, :, LaTeX, or, InDesign, ?]...</td>\n",
       "      <td>ask programming: latex or indesign?: submitted...</td>\n",
       "      <td>[http://tinyurl.com/myfmf7]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ask programming latex or indesign submitted by...</td>\n",
       "      <td>9</td>\n",
       "      <td>[ask programming latex or indesign submitted b...</td>\n",
       "      <td>[ask, programming, latex, or, indesign, submit...</td>\n",
       "      <td>[ask, programming, latex, indesign, submitted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "      <td>[On that note, I hate Word., I hate Pages., I ...</td>\n",
       "      <td>[[On, that, note, ,, I, hate, Word, .], [I, ha...</td>\n",
       "      <td>on that note, i hate word. i hate pages. i hat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>on that note i hate word i hate pages i hate l...</td>\n",
       "      <td>9</td>\n",
       "      <td>[on that note i hate word i hate pages i hate ...</td>\n",
       "      <td>[on, that, note, i, hate, word, i, hate, pages...</td>\n",
       "      <td>[note, hate, word, hate, pages, hate, latex, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[Ahhh... back in a *real* text editing environ...</td>\n",
       "      <td>[[Ahhh, ..., back, in, a, *, real, *, text, ed...</td>\n",
       "      <td>ahhh... back in a *real* text editing environm...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>ahhh back in a real text editing environment i...</td>\n",
       "      <td>10</td>\n",
       "      <td>[ahhh back in a real text editing environment ...</td>\n",
       "      <td>[ahhh, back, in, a, real, text, editing, envir...</td>\n",
       "      <td>[ahhh, back, real, text, editing, environment,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "      <td>[Trouble in Iran, I see., Hmm., Iran., Iran so...</td>\n",
       "      <td>[[Trouble, in, Iran, ,, I, see, .], [Hmm, .], ...</td>\n",
       "      <td>trouble in iran, i see. hmm. iran. iran so far...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>trouble in iran i see hmm iran iran so far awa...</td>\n",
       "      <td>6</td>\n",
       "      <td>[trouble in iran i see hmm iran iran so far aw...</td>\n",
       "      <td>[trouble, in, iran, i, see, hmm, iran, iran, s...</td>\n",
       "      <td>[trouble, iran, see, hmm, iran, iran, far, awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "      <td>[Reading the tweets coming out of Iran..., The...</td>\n",
       "      <td>[[Reading, the, tweets, coming, out, of, Iran,...</td>\n",
       "      <td>reading the tweets coming out of iran... the w...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>reading the tweets coming out of iran the whol...</td>\n",
       "      <td>6</td>\n",
       "      <td>[reading the tweets coming out of iran the who...</td>\n",
       "      <td>[reading, the, tweets, coming, out, of, iran, ...</td>\n",
       "      <td>[reading, tweets, coming, iran, whole, thing, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600498 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label       Text1                     TimeStamp     Meta1  \\\n",
       "0            0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "1600493      2       14072  Sun Jun 14 04:31:43 UTC 2009     latex   \n",
       "1600494      0       14073  Sun Jun 14 04:32:17 UTC 2009     latex   \n",
       "1600495      4       14074  Sun Jun 14 04:36:34 UTC 2009     latex   \n",
       "1600496      0       14075  Sun Jun 14 21:36:07 UTC 2009      iran   \n",
       "1600497      0       14076  Sun Jun 14 21:36:17 UTC 2009      iran   \n",
       "\n",
       "                   Meta2                                           MainText  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1600493          proggit  Ask Programming: LaTeX or InDesign?: submitted...   \n",
       "1600494           sam33r  On that note, I hate Word. I hate Pages. I hat...   \n",
       "1600495  iamtheonlyjosie  Ahhh... back in a *real* text editing environm...   \n",
       "1600496        plutopup7  Trouble in Iran, I see. Hmm. Iran. Iran so far...   \n",
       "1600497     captain_pete  Reading the tweets coming out of Iran... The w...   \n",
       "\n",
       "                                before_sent_tokenized_text  \\\n",
       "0        [@switchfoot http://twitpic.com/2y1zl - Awww, ...   \n",
       "1        [is upset that he can't update his Facebook by...   \n",
       "2        [@Kenichan I dived many times for the ball., M...   \n",
       "3         [my whole body feels itchy and like its on fire]   \n",
       "4        [@nationwideclass no, it's not behaving at all...   \n",
       "...                                                    ...   \n",
       "1600493  [Ask Programming: LaTeX or InDesign?, : submit...   \n",
       "1600494  [On that note, I hate Word., I hate Pages., I ...   \n",
       "1600495  [Ahhh... back in a *real* text editing environ...   \n",
       "1600496  [Trouble in Iran, I see., Hmm., Iran., Iran so...   \n",
       "1600497  [Reading the tweets coming out of Iran..., The...   \n",
       "\n",
       "                                before_word_tokenized_text  \\\n",
       "0        [[@, switchfoot, http, :, //twitpic.com/2y1zl,...   \n",
       "1        [[is, upset, that, he, ca, n't, update, his, F...   \n",
       "2        [[@, Kenichan, I, dived, many, times, for, the...   \n",
       "3        [[my, whole, body, feels, itchy, and, like, it...   \n",
       "4        [[@, nationwideclass, no, ,, it, 's, not, beha...   \n",
       "...                                                    ...   \n",
       "1600493  [[Ask, Programming, :, LaTeX, or, InDesign, ?]...   \n",
       "1600494  [[On, that, note, ,, I, hate, Word, .], [I, ha...   \n",
       "1600495  [[Ahhh, ..., back, in, a, *, real, *, text, ed...   \n",
       "1600496  [[Trouble, in, Iran, ,, I, see, .], [Hmm, .], ...   \n",
       "1600497  [[Reading, the, tweets, coming, out, of, Iran,...   \n",
       "\n",
       "                                              cleaned_text  \\\n",
       "0        @switchfoot  - awww, that's a bummer.  you sho...   \n",
       "1        is upset that he can't update his facebook by ...   \n",
       "2        @kenichan i dived many times for the ball. man...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        @nationwideclass no, it's not behaving at all....   \n",
       "...                                                    ...   \n",
       "1600493  ask programming: latex or indesign?: submitted...   \n",
       "1600494  on that note, i hate word. i hate pages. i hat...   \n",
       "1600495  ahhh... back in a *real* text editing environm...   \n",
       "1600496  trouble in iran, i see. hmm. iran. iran so far...   \n",
       "1600497  reading the tweets coming out of iran... the w...   \n",
       "\n",
       "                         URL_address   detected_account detected_phone_number  \\\n",
       "0         [http://twitpic.com/2y1zl]       [switchfoot]                    []   \n",
       "1                                 []                 []                    []   \n",
       "2                                 []         [kenichan]                    []   \n",
       "3                                 []                 []                    []   \n",
       "4                                 []  [nationwideclass]                    []   \n",
       "...                              ...                ...                   ...   \n",
       "1600493  [http://tinyurl.com/myfmf7]                 []                    []   \n",
       "1600494                           []                 []                    []   \n",
       "1600495                           []                 []                    []   \n",
       "1600496                           []                 []                    []   \n",
       "1600497                           []                 []                    []   \n",
       "\n",
       "                                 special_character_removed  \\\n",
       "0        switchfoot   awww thats a bummer  you shoulda ...   \n",
       "1        is upset that he cant update his facebook by t...   \n",
       "2        kenichan i dived many times for the ball manag...   \n",
       "3          my whole body feels itchy and like its on fire    \n",
       "4        nationwideclass no its not behaving at all im ...   \n",
       "...                                                    ...   \n",
       "1600493  ask programming latex or indesign submitted by...   \n",
       "1600494  on that note i hate word i hate pages i hate l...   \n",
       "1600495  ahhh back in a real text editing environment i...   \n",
       "1600496  trouble in iran i see hmm iran iran so far awa...   \n",
       "1600497  reading the tweets coming out of iran the whol...   \n",
       "\n",
       "         special_characters_detected  \\\n",
       "0                                  7   \n",
       "1                                  6   \n",
       "2                                  5   \n",
       "3                                  0   \n",
       "4                                  9   \n",
       "...                              ...   \n",
       "1600493                            9   \n",
       "1600494                            9   \n",
       "1600495                           10   \n",
       "1600496                            6   \n",
       "1600497                            6   \n",
       "\n",
       "                                       sent_tokenized_text  \\\n",
       "0        [switchfoot   awww thats a bummer  you shoulda...   \n",
       "1        [is upset that he cant update his facebook by ...   \n",
       "2        [kenichan i dived many times for the ball mana...   \n",
       "3         [my whole body feels itchy and like its on fire]   \n",
       "4        [nationwideclass no its not behaving at all im...   \n",
       "...                                                    ...   \n",
       "1600493  [ask programming latex or indesign submitted b...   \n",
       "1600494  [on that note i hate word i hate pages i hate ...   \n",
       "1600495  [ahhh back in a real text editing environment ...   \n",
       "1600496  [trouble in iran i see hmm iran iran so far aw...   \n",
       "1600497  [reading the tweets coming out of iran the who...   \n",
       "\n",
       "                                       word_tokenized_text  \\\n",
       "0        [switchfoot, awww, thats, a, bummer, you, shou...   \n",
       "1        [is, upset, that, he, cant, update, his, faceb...   \n",
       "2        [kenichan, i, dived, many, times, for, the, ba...   \n",
       "3        [my, whole, body, feels, itchy, and, like, its...   \n",
       "4        [nationwideclass, no, its, not, behaving, at, ...   \n",
       "...                                                    ...   \n",
       "1600493  [ask, programming, latex, or, indesign, submit...   \n",
       "1600494  [on, that, note, i, hate, word, i, hate, pages...   \n",
       "1600495  [ahhh, back, in, a, real, text, editing, envir...   \n",
       "1600496  [trouble, in, iran, i, see, hmm, iran, iran, s...   \n",
       "1600497  [reading, the, tweets, coming, out, of, iran, ...   \n",
       "\n",
       "                                    stopwords_removed_text  \n",
       "0        [switchfoot, awww, thats, bummer, shoulda, got...  \n",
       "1        [upset, cant, update, facebook, texting, might...  \n",
       "2        [kenichan, dived, many, times, ball, managed, ...  \n",
       "3                  [whole, body, feels, itchy, like, fire]  \n",
       "4          [nationwideclass, behaving, im, mad, cant, see]  \n",
       "...                                                    ...  \n",
       "1600493  [ask, programming, latex, indesign, submitted,...  \n",
       "1600494  [note, hate, word, hate, pages, hate, latex, s...  \n",
       "1600495  [ahhh, back, real, text, editing, environment,...  \n",
       "1600496  [trouble, iran, see, hmm, iran, iran, far, awa...  \n",
       "1600497  [reading, tweets, coming, iran, whole, thing, ...  \n",
       "\n",
       "[1600498 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words_before = df['word_tokenized_text'].apply(len).sum()\n",
    "\n",
    "print(\"Total words before removing stopwords:\", total_words_before)\n",
    "#Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['stopwords_removed_text'] = df['word_tokenized_text'].apply(\n",
    "    lambda x: [word for word in x if word not in stop_words]\n",
    ")\n",
    "stopwords_removed = total_words_before - df['stopwords_removed_text'].apply(len).sum()\n",
    "print(\"Stopwords removed:\", stopwords_removed)\n",
    "after_stopwords = df['stopwords_removed_text'].apply(len).sum()\n",
    "print(\"Total words after removing stopwords:\", after_stopwords)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package for stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stemming and lemmatization\n",
    "nltk.download('wordnet', download_dir='/usr/local/share/nltk_data')\n",
    "from nltk.stem import WordNetLemmatizer #For lemmatization\n",
    "from nltk.stem import PorterStemmer #For Steming \n",
    "from nltk.corpus import wordnet #For WordNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [switchfoot, awww, thats, bummer, shoulda, got...\n",
       "1          [upset, cant, update, facebook, texting, might...\n",
       "2          [kenichan, dived, many, time, ball, managed, s...\n",
       "3                     [whole, body, feel, itchy, like, fire]\n",
       "4            [nationwideclass, behaving, im, mad, cant, see]\n",
       "                                 ...                        \n",
       "1600493    [ask, programming, latex, indesign, submitted,...\n",
       "1600494    [note, hate, word, hate, page, hate, latex, sa...\n",
       "1600495    [ahhh, back, real, text, editing, environment,...\n",
       "1600496    [trouble, iran, see, hmm, iran, iran, far, awa...\n",
       "1600497    [reading, tweet, coming, iran, whole, thing, t...\n",
       "Name: lemmatized_text, Length: 1600498, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization to each word in the list\n",
    "df['lemmatized_text'] =df['stopwords_removed_text'].apply(\n",
    "    lambda wordlist: [lemmatizer.lemmatize(word) for word in wordlist]\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "df['lemmatized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [switchfoot, awww, that, bummer, shoulda, got,...\n",
       "1          [upset, cant, updat, facebook, text, might, cr...\n",
       "2          [kenichan, dive, mani, time, ball, manag, save...\n",
       "3                     [whole, bodi, feel, itchi, like, fire]\n",
       "4               [nationwideclass, behav, im, mad, cant, see]\n",
       "                                 ...                        \n",
       "1600493    [ask, program, latex, indesign, submit, calcio...\n",
       "1600494    [note, hate, word, hate, page, hate, latex, sa...\n",
       "1600495    [ahhh, back, real, text, edit, environ, lt, la...\n",
       "1600496    [troubl, iran, see, hmm, iran, iran, far, away...\n",
       "1600497    [read, tweet, come, iran, whole, thing, terrif...\n",
       "Name: stemmed_text, Length: 1600498, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming \n",
    "#Intialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Apply stemming to each word in the list\n",
    "df['stemmed_text'] = df['lemmatized_text'].apply(\n",
    "    lambda wordlist: [stemmer.stem(word) for word in wordlist]\n",
    "    \n",
    ")\n",
    "\n",
    "#check the result \n",
    "df['stemmed_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total notebook runtime: 3.14 minutes\n"
     ]
    }
   ],
   "source": [
    "# End timing the whole notebook\n",
    "notebook_end_time = time.time()\n",
    "\n",
    "# Calculate the total runtime\n",
    "notebook_runtime = (notebook_end_time - notebook_start_time) / 60  # Convert seconds to minutes\n",
    "print(f\"Total notebook runtime: {notebook_runtime:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics\n",
    "- Number of documents:\n",
    "- Average sentence length: \n",
    "- Word count: \n",
    "- Sentence count: \n",
    "- Vocabulary size: \n",
    "- Max word length:\n",
    "- Min sentence length: \n",
    "- Max sentence length: \n",
    "- Special characters removed:\n",
    "- Stop words removed:\n",
    "- Addresses detected:\n",
    "- Phone numbers detected:\n",
    "- Account numbers detected: \n",
    "- Total runtime: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Text Cleaning Statistics ###\n",
      "\n",
      "- Number of documents: 1600498 → 1600498\n",
      "- Average sentence length: 1.7 → 55.3\n",
      "- Word count: 26261279 → 88585053\n",
      "- Sentence count: 2748402 → 1600498\n",
      "- Vocabulary size: 874726 → 26\n",
      "- Max word length: 204 → 125\n",
      "- Min sentence length: 1 → 1\n",
      "- Max sentence length: 229 → 41\n",
      "- Special characters removed: 6876608\n",
      "- Stop words removed: 8371773\n",
      "- Addresses detected: 71722\n",
      "- Phone numbers detected: 1215\n",
      "- Account numbers detected: 786705\n",
      "- Total runtime: 3.14 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display text cleaning statistics\n",
    "before_stats = {\n",
    "    \"doc_count\": number_of_documents,\n",
    "    \"avg_sentence_length\": before_avg_sentences_per_doc,\n",
    "    \"word_count\": before_number_of_words,\n",
    "    \"sentence_count\": before_number_of_sentences,\n",
    "    \"vocabulary_size\": before_unique_words,\n",
    "    \"max_word_length\": before_max_word_length,\n",
    "    \"min_sentence_length\": before_min_sentence_length,\n",
    "    \"max_sentence_length\": before_max_sentence_length,\n",
    "}\n",
    "\n",
    "after_stats = {\n",
    "    \"doc_count\": len(df),\n",
    "    \"avg_sentence_length\": avg_sentence_length,\n",
    "    \"word_count\": word_count,\n",
    "    \"sentence_count\": sentence_count,\n",
    "    \"vocabulary_size\": vocabulary_size_entire_dataset,\n",
    "    \"max_word_length\": max_word_length,\n",
    "    \"min_sentence_length\": min_sentence_length,\n",
    "    \"max_sentence_length\": max_sentence_length,\n",
    "}\n",
    "\n",
    "cleaning_metrics = {\n",
    "    \"special_chars_removed\": total_special_characters_detected,\n",
    "    \"stop_words_removed\": stopwords_removed,\n",
    "    \"addresses_detected\": total_urls,\n",
    "    \"phones_detected\": total_phone_numbers,\n",
    "    \"accounts_detected\": total_accounts,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "### Text Cleaning Statistics ###\n",
    "\n",
    "- Number of documents: {before_stats['doc_count']} → {after_stats['doc_count']}\n",
    "- Average sentence length: {before_stats['avg_sentence_length']:.1f} → {after_stats['avg_sentence_length']:.1f}\n",
    "- Word count: {before_stats['word_count']} → {after_stats['word_count']}\n",
    "- Sentence count: {before_stats['sentence_count']} → {after_stats['sentence_count']}\n",
    "- Vocabulary size: {before_stats['vocabulary_size']} → {after_stats['vocabulary_size']}\n",
    "- Max word length: {before_stats['max_word_length']} → {after_stats['max_word_length']}\n",
    "- Min sentence length: {before_stats['min_sentence_length']} → {after_stats['min_sentence_length']}\n",
    "- Max sentence length: {before_stats['max_sentence_length']} → {after_stats['max_sentence_length']}\n",
    "- Special characters removed: {cleaning_metrics['special_chars_removed']}\n",
    "- Stop words removed: {cleaning_metrics['stop_words_removed']}\n",
    "- Addresses detected: {cleaning_metrics['addresses_detected']}\n",
    "- Phone numbers detected: {cleaning_metrics['phones_detected']}\n",
    "- Account numbers detected: {cleaning_metrics['accounts_detected']}\n",
    "- Total runtime: {notebook_runtime:.2f} minutes\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
